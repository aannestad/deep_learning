{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Hans Martin Aannestad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tests_backpropagation import main_test\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class ``MyNet``\n",
    "\n",
    "Read carefully how ``MyNet`` is implemented in the cell below. In particular:  \n",
    "- ``n_hid`` is a list of integer, representing the number of hidden units in each hidden layer.   \n",
    "-  ``MyNet([2, 3, 2]) = MiniNet()`` where ``MiniNet`` is the neural network defined in the fourth tutorial, in which notations are also clarified.     \n",
    "- ``model.L`` is the number of hidden layers, ``L``   \n",
    "- ``model.f[l]`` is the activation function of layer ``l``, $f^{[l]}$ (here ``torch.tanh``)   \n",
    "- ``model.df[l]`` is the derivative of the activation function, $f'^{[l]}$   \n",
    "- ``model.a[l]``  is the tensor $A^{[l]}$, (shape: ``(1, n(l))``)   \n",
    "- ``model.z[l]``  is the tensor $Z^{[l]}$, (shape: ``(1, n(l))``)  \n",
    "- Weights $W^{[l]}$ (shape: ``(n(l+1), n(l))``) and biases $\\mathbf{b}^{[l]}$ (shape: ``(n(l+1))``) can be accessed as follows:\n",
    "```\n",
    "weights = model.fc[str(l)].weight.data\n",
    "bias = model.fc[str(l)].bias.data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self, n_l = [2, 3, 2]):\n",
    "        super().__init__() \n",
    "        \n",
    "        \n",
    "        # number of layers in our network (following Andrew's notations)\n",
    "        self.L = len(n_l)-1\n",
    "        self.n_l = n_l\n",
    "        \n",
    "        # Where we will store our neuron values\n",
    "        # - z: before activation function \n",
    "        # - a: after activation function (a=f(z))\n",
    "        self.z = {i : None for i in range(1, self.L+1)}\n",
    "        self.a = {i : None for i in range(self.L+1)}\n",
    "\n",
    "        # Where we will store the gradients for our custom backpropagation algo\n",
    "        self.dL_dw = {i : None for i in range(1, self.L+1)}\n",
    "        self.dL_db = {i : None for i in range(1, self.L+1)}\n",
    "\n",
    "        # Our activation functions\n",
    "        self.f = {i : lambda x : torch.tanh(x) for i in range(1, self.L+1)}\n",
    "\n",
    "        # Derivatives of our activation functions\n",
    "        self.df = {\n",
    "            i : lambda x : (1 / (torch.cosh(x)**2)) \n",
    "            for i in range(1, self.L+1)\n",
    "        }\n",
    "        \n",
    "        # fully connected layers\n",
    "        # We have to use nn.ModuleDict and to use strings as keys here to \n",
    "        # respect pytorch requirements (otherwise, the model does not learn)\n",
    "        self.fc = nn.ModuleDict({str(i): None for i in range(1, self.L+1)})\n",
    "        for i in range(1, self.L+1):\n",
    "            self.fc[str(i)] = nn.Linear(in_features=n_l[i-1], out_features=n_l[i])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input layer\n",
    "        self.a[0] = torch.flatten(x, 1)\n",
    "        \n",
    "        # Hidden layers until output layer\n",
    "        for i in range(1, self.L+1):\n",
    "\n",
    "            # fully connected layer\n",
    "            self.z[i] = self.fc[str(i)](self.a[i-1])\n",
    "            # activation\n",
    "            self.a[i] = self.f[i](self.z[i])\n",
    "\n",
    "        # return output\n",
    "        return self.a[self.L]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "Write a function ``backpropagation(model, y_true, y_pred)`` that computes:\n",
    "\n",
    "- $\\frac{\\partial L}{\\partial w^{[l]}_{i,j}}$ and store them in ``model.dL_dw[l][i,j]`` for $l \\in [1 .. L]$ \n",
    "- $\\frac{\\partial L}{\\partial b^{[l]}_{j}}$ and store them in ``model.dL_db[l][j]`` for $l \\in [1 .. L]$ \n",
    "\n",
    "assuming ``model`` is an instance of the ``MyNet`` class.\n",
    "\n",
    "A vectorized implementation would be appreciated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some testing\n",
    "\n",
    "#print(model)\n",
    "#model.df[str(L)](model.z[str(L)].data).data\n",
    "#L = len(model.dL_dw)\n",
    "#print(L)\n",
    "#print(model.df[2](model.z[2].data).data)\n",
    "#layer_name = 'fc' + str(2)\n",
    "#for param_name in ['weight', 'bias']:\n",
    "#getattr(model.fc[str(1)], 'weight').data\n",
    "#model.fc[str(2)].weight.data\n",
    "#model.z[1].data\n",
    "#(model.z[1])\n",
    "#getattr(model.fc[str(1)], 'weight').data\n",
    "#print(f'bias: {model.fc[str(l)].bias.data}')\n",
    "#print(f'dbias: {model.dL_db[l].data}')\n",
    "#print(f'weights: {model.fc[str(l)].weight.data}')\n",
    "#print(f'dweights: {model.dL_dw[l].data}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(model, y_true, y_pred):\n",
    "    # (Vectorization encouraged: I used matmul -> not loops)\n",
    "    # Nested: model.fc[model.L]\n",
    "\n",
    "    # Prevent Pytorch from computing and keeping track of gradients\n",
    "    with torch.no_grad(): # not needed here\n",
    "    \n",
    "    # MSE will be Sum(-(y_true - y_pred)**2)/len(y_true)\n",
    "    # d_MSE is therefore: Sum(-2*(y_true - y_pred))/len(y_true), use in last layer by:\n",
    "    \n",
    "        d_current = (-2*(y_true - y_pred)**(2-1)) * model.df[model.L](model.z[model.L])\n",
    "\n",
    "        model.dL_dw[model.L] = torch.t(torch.matmul(torch.t(model.a[model.L-1]), d_current))\n",
    "        model.dL_db[model.L] = d_current[0] # last layer bias\n",
    "    \n",
    "        n_layers = model.L-1\n",
    "\n",
    "        for layer in range(n_layers, 0, -1): # reverse (back)propagation loop\n",
    "    \n",
    "            l_n = str(layer+1) # next layer (string)\n",
    "            d_current = torch.t(torch.matmul(torch.t(model.fc[l_n].weight.data), torch.t(d_current))) * model.df[layer](model.z[layer])\n",
    "        \n",
    "            model.dL_dw[layer] = torch.t(torch.matmul(torch.t(model.a[layer-1]),d_current))\n",
    "            model.dL_db[layer] = d_current[0]\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(getattr(model.fc['2'], 'weight').requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the cells below, and check the output\n",
    "\n",
    "- In the 1st cell, we use a toy dataset and the same architecture as the MiniNet class of the fourth tutorial. \n",
    "- In the 2nd cell, we use a few samples of the MNIST dataset with a consistent model architecture (``24x24`` black and white cropped images as input and ``10`` output classes). \n",
    "\n",
    "You can set ``verbose`` to ``True`` if you want more details about your computations versus what is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ========================  Check gradients ======================== \n",
      "\n",
      " ================= Epoch 1 ================= \n",
      "\n",
      " ------------ fc['1'].weight.grad ------------ \n",
      "  Our computation:\n",
      " tensor([[-4.1753e-07, -3.9125e-07],\n",
      "        [-1.7184e-03, -1.6103e-03],\n",
      "        [ 5.7746e-05,  5.4111e-05]])\n",
      "  Autograd's computation:\n",
      " tensor([[-4.1753e-07, -3.9125e-07],\n",
      "        [-1.7184e-03, -1.6103e-03],\n",
      "        [ 5.7746e-05,  5.4111e-05]])\n",
      "\n",
      " ------------- fc['1'].bias.grad ------------- \n",
      "  Our computation:\n",
      " tensor([-4.3507e-08, -1.7906e-04,  6.0171e-06])\n",
      "  Autograd's computation:\n",
      " tensor([-4.3507e-08, -1.7906e-04,  6.0171e-06])\n",
      "\n",
      " ------------- relative error ------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0000\n",
      "(fc[1].bias.grad, model.dL_db[1]):   0.0000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad, model.dL_db[2]):   0.0000\n",
      "Gradients consistent with finite differences computations. :) \n",
      "\n",
      " ================= Epoch 2 ================= \n",
      "\n",
      " ------------ fc['1'].weight.grad ------------ \n",
      "  Our computation:\n",
      " tensor([[-2.8201e-07, -2.6425e-07],\n",
      "        [-8.3564e-04, -7.8304e-04],\n",
      "        [ 3.0540e-05,  2.8618e-05]])\n",
      "  Autograd's computation:\n",
      " tensor([[-2.8201e-07, -2.6425e-07],\n",
      "        [-8.3564e-04, -7.8304e-04],\n",
      "        [ 3.0540e-05,  2.8618e-05]])\n",
      "\n",
      " ------------- fc['1'].bias.grad ------------- \n",
      "  Our computation:\n",
      " tensor([-2.9385e-08, -8.7074e-05,  3.1823e-06])\n",
      "  Autograd's computation:\n",
      " tensor([-2.9385e-08, -8.7074e-05,  3.1823e-06])\n",
      "\n",
      " ------------- relative error ------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0000\n",
      "(fc[1].bias.grad, model.dL_db[1]):   0.0000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad, model.dL_db[2]):   0.0000\n",
      "Gradients consistent with finite differences computations. :) \n",
      "\n",
      " ================= Epoch 3 ================= \n",
      "\n",
      " ------------ fc['1'].weight.grad ------------ \n",
      "  Our computation:\n",
      " tensor([[-2.5506e-07, -2.3900e-07],\n",
      "        [-5.9882e-04, -5.6113e-04],\n",
      "        [ 2.3549e-05,  2.2067e-05]])\n",
      "  Autograd's computation:\n",
      " tensor([[-2.5506e-07, -2.3900e-07],\n",
      "        [-5.9882e-04, -5.6113e-04],\n",
      "        [ 2.3549e-05,  2.2067e-05]])\n",
      "\n",
      " ------------- fc['1'].bias.grad ------------- \n",
      "  Our computation:\n",
      " tensor([-2.6577e-08, -6.2397e-05,  2.4538e-06])\n",
      "  Autograd's computation:\n",
      " tensor([-2.6577e-08, -6.2397e-05,  2.4538e-06])\n",
      "\n",
      " ------------- relative error ------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0000\n",
      "(fc[1].bias.grad, model.dL_db[1]):   0.0000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad, model.dL_db[2]):   0.0000\n",
      "Gradients consistent with finite differences computations. :) \n",
      "\n",
      " ================= Epoch 4 ================= \n",
      "\n",
      " ------------ fc['1'].weight.grad ------------ \n",
      "  Our computation:\n",
      " tensor([[-2.5057e-07, -2.3480e-07],\n",
      "        [-4.9091e-04, -4.6001e-04],\n",
      "        [ 2.1200e-05,  1.9865e-05]])\n",
      "  Autograd's computation:\n",
      " tensor([[-2.5057e-07, -2.3480e-07],\n",
      "        [-4.9091e-04, -4.6001e-04],\n",
      "        [ 2.1200e-05,  1.9865e-05]])\n",
      "\n",
      " ------------- fc['1'].bias.grad ------------- \n",
      "  Our computation:\n",
      " tensor([-2.6110e-08, -5.1153e-05,  2.2090e-06])\n",
      "  Autograd's computation:\n",
      " tensor([-2.6110e-08, -5.1153e-05,  2.2090e-06])\n",
      "\n",
      " ------------- relative error ------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0000\n",
      "(fc[1].bias.grad, model.dL_db[1]):   0.0000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad, model.dL_db[2]):   0.0000\n",
      "Gradients consistent with finite differences computations. :) \n",
      "\n",
      " ================= Epoch 5 ================= \n",
      "\n",
      " ------------ fc['1'].weight.grad ------------ \n",
      "  Our computation:\n",
      " tensor([[-2.2734e-07, -2.1303e-07],\n",
      "        [-3.9205e-04, -3.6737e-04],\n",
      "        [ 1.9654e-05,  1.8416e-05]])\n",
      "  Autograd's computation:\n",
      " tensor([[-2.2734e-07, -2.1303e-07],\n",
      "        [-3.9205e-04, -3.6737e-04],\n",
      "        [ 1.9654e-05,  1.8416e-05]])\n",
      "\n",
      " ------------- fc['1'].bias.grad ------------- \n",
      "  Our computation:\n",
      " tensor([-2.3689e-08, -4.0852e-05,  2.0479e-06])\n",
      "  Autograd's computation:\n",
      " tensor([-2.3689e-08, -4.0852e-05,  2.0479e-06])\n",
      "\n",
      " ------------- relative error ------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0000\n",
      "(fc[1].bias.grad, model.dL_db[1]):   0.0000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad, model.dL_db[2]):   0.0000\n",
      "Gradients consistent with finite differences computations. :) \n",
      "\n",
      " ==============  Check that weights have been updated ============= \n",
      "tensor([[ 0.4056,  0.5568],\n",
      "        [-0.2396,  0.7361],\n",
      "        [-0.3610, -0.2946]])\n",
      "tensor([-0.5659,  0.2033, -0.0111])\n",
      "Weights have been updated. :)\n",
      "\n",
      " ===================  Check computational graph =================== \n",
      "All parameters seem correctly attached to the computational graph! :) \n"
     ]
    }
   ],
   "source": [
    "model = MyNet([2, 3, 2])\n",
    "main_test(backpropagation, model, verbose=True, data='toy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ========================  Check gradients ======================== \n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hmaan\\anaconda3\\envs\\ai\\lib\\site-packages\\torch\\nn\\modules\\loss.py:520: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "\n",
      " ================= Epoch 1 ================= \n",
      "Gradients consistent with autograd's computations. :) \n",
      "Gradients consistent with finite differences computations. :) \n",
      "\n",
      " ==============  Check that weights have been updated ============= \n",
      "Weights have been updated. :)\n",
      "\n",
      " ===================  Check computational graph =================== \n",
      "All parameters seem correctly attached to the computational graph! :) \n"
     ]
    }
   ],
   "source": [
    "model = MyNet([24*24, 16, 10])\n",
    "main_test(backpropagation, model, verbose=True, data='mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('nglm-env': conda)",
   "metadata": {
    "interpreter": {
     "hash": "ae36e8c2cbd9e14d80419493f2540eab6c211be174ac39ce04705a74740d0d8b"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

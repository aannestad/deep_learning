{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W70DJc_dJh5k"
      },
      "source": [
        "## 3 Gradient Descent\n",
        "\n",
        "## By Hans Martin Aannestad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4Mwa5gCJh5s"
      },
      "source": [
        "1. Load and preprocess the CIFAR-10 dataset. Split it into 3 datasets: training, validation and\n",
        "test. Take a subset of these datasets by keeping only 2 labels: bird and plane."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1RbyjXJJh5v",
        "outputId": "90d4eebd-a9d5-4fdd-965f-69c548572a0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on device cpu.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import random_split\n",
        "from datetime import datetime\n",
        "\n",
        "torch.manual_seed(265)\n",
        "\n",
        "batch_size =  256\n",
        "n_epoch =  30\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "seed =  265\n",
        "\n",
        "\n",
        "device = (torch.device('cuda') if torch.cuda.is_available()\n",
        "          else torch.device('cpu'))\n",
        "print(f\"Training on device {device}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7H1xQNJJJh52",
        "outputId": "403be33c-e986-47de-afe4-a67b4636fdcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Size of the train dataset:         45000\n",
            "Size of the validation dataset:    5000\n",
            "Size of the test dataset:          10000\n",
            "Size of the training dataset:  9017\n",
            "Size of the validation dataset:  983\n",
            "Size of the test dataset:  2000\n"
          ]
        }
      ],
      "source": [
        "def load_cifar(train_val_split=0.9, data_path='../data/', preprocessor=None):\n",
        "    \n",
        "    # Define preprocessor if not already given\n",
        "    if preprocessor is None:\n",
        "        preprocessor = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                                (0.2470, 0.2435, 0.2616))\n",
        "        ])\n",
        "    \n",
        "    # load datasets\n",
        "    data_train_val = datasets.CIFAR10(\n",
        "        data_path,       \n",
        "        train=True,      \n",
        "        download=True,\n",
        "        transform=preprocessor)\n",
        "\n",
        "    data_test = datasets.CIFAR10(\n",
        "        data_path, \n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=preprocessor)\n",
        "\n",
        "    # train/validation split\n",
        "    n_train = int(len(data_train_val)*train_val_split)\n",
        "    n_val =  len(data_train_val) - n_train\n",
        "\n",
        "    data_train, data_val = random_split(\n",
        "        data_train_val, \n",
        "        [n_train, n_val],\n",
        "        generator=torch.Generator().manual_seed(123)\n",
        "    )\n",
        "\n",
        "    print(\"Size of the train dataset:        \", len(data_train))\n",
        "    print(\"Size of the validation dataset:   \", len(data_val))\n",
        "    print(\"Size of the test dataset:         \", len(data_test))\n",
        "    \n",
        "    return (data_train, data_val, data_test)\n",
        "\n",
        "cifar10_train, cifar10_val, cifar10_test = load_cifar()\n",
        "\n",
        "# Now define a lighter version of CIFAR10: cifar\n",
        "label_map = {0: 0, 2: 1}\n",
        "class_names = ['airplane', 'bird']\n",
        "\n",
        "# For each dataset, keep only airplanes and birds\n",
        "cifar2_train = [(img, label_map[label]) for img, label in cifar10_train if label in [0, 2]]\n",
        "cifar2_val = [(img, label_map[label]) for img, label in cifar10_val if label in [0, 2]]\n",
        "cifar2_test = [(img, label_map[label]) for img, label in cifar10_test if label in [0, 2]]\n",
        "\n",
        "print('Size of the training dataset: ', len(cifar2_train))\n",
        "print('Size of the validation dataset: ', len(cifar2_val))\n",
        "print('Size of the test dataset: ', len(cifar2_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpbvVGI2Jh58"
      },
      "source": [
        "2. Write a MyMLP class that implements a MLP in PyTorch (so only fully connected layers) such that:\n",
        "\n",
        "(a) The input dimension is 3072 (= 32*32*3) and the output dimension is 2 (for the 2\n",
        "classes).\n",
        "\n",
        "(b) The hidden layers have respectively 512, 128 and 32 hidden units.\n",
        "\n",
        "(c) All activation functions are ReLU. The last layer has no activation function since the\n",
        "cross-entropy loss already includes a softmax activation function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LhwfnH6DJh5_"
      },
      "outputs": [],
      "source": [
        "class MyMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()  # to inherit the '__init__' method from the 'nn.Module' class\n",
        "        # Add whatever you want here (e.g layers and activation functions)\n",
        "        # The order and names don't matter here but it is easier to understand\n",
        "        # if you go for Layer1, fun1, layer2, fun2, etc\n",
        "        # Some conventions:\n",
        "  \n",
        "        # - fc for fully connected\n",
        "\n",
        "        self.flat = nn.Flatten()\n",
        "        # 32*32*3: determined by our dataset: 32x32 RGB images\n",
        "        self.fc0 = nn.Linear(32*32*3, 512)\n",
        "        self.act0 = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(512,128)\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 32)\n",
        "        self.act2 = nn.ReLU()\n",
        "        # 2: determined by our number of classes (birds and planes)\n",
        "        self.fc3 = nn.Linear(32, 2) # no activation since cross-entropy loss\n",
        "        \n",
        "    # Remember, we saw earlier that `forward` defines the \n",
        "    # computation performed at every call (the forward pass) and that it\n",
        "    # should be overridden by all subclasses.\n",
        "    def forward(self, x):\n",
        "        # Now the order matters! \n",
        "        out = self.flat(x)\n",
        "        out = self.act0(self.fc0(out))\n",
        "        out = self.act1(self.fc1(out))\n",
        "        out = self.act2(self.fc2(out))\n",
        "        out = self.fc3(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "293cyznpJh6C"
      },
      "source": [
        "3. Write a train(n epochs, optimizer, model, loss fn, train loader) function that trains\n",
        "model for n epochs epochs given an optimizer optimizer, a loss function loss fn and a dataloader train loader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CiRgU7EbJh6D"
      },
      "outputs": [],
      "source": [
        "def train(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "    \n",
        "    n_batch = len(train_loader)\n",
        "    losses_train = []\n",
        "    model.train()\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    \n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        \n",
        "        loss_train = 0.0\n",
        "        for imgs, labels in train_loader:\n",
        "\n",
        "            imgs = imgs.to(device=device) \n",
        "            labels = labels.to(device=device)\n",
        "\n",
        "            outputs = model(imgs)\n",
        "            \n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            \n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss_train += loss.item()\n",
        "            \n",
        "        losses_train.append(loss_train / n_batch)\n",
        "\n",
        "        if epoch == 1 or epoch % 5 == 0:\n",
        "            print('{}  |  Epoch {}  |  Training loss {:.3f}'.format(\n",
        "                datetime.now().time(), epoch, loss_train / n_batch))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mot_dmhOJh6G"
      },
      "source": [
        "4. Write a similar function train manual update that has no optimizer parameter, but a learning rate lr parameter instead and that manually updates each trainable parameter of model\n",
        "using equation (3). Do not forget to zero out all gradients after each iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NDeBwjbYJh6I"
      },
      "outputs": [],
      "source": [
        "def train_manual(n_epochs, model, loss_fn, train_loader, lr=1e-2):\n",
        "    model.train()\n",
        "    \n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss_train = 0.0\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs = imgs.to(device=device) \n",
        "            labels = labels.to(device=device)\n",
        "\n",
        "            outputs = model(imgs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for i, p in enumerate(model.parameters()):\n",
        "                    if p.grad is not None:\n",
        "                        grad = p.grad.detach()\n",
        "                        p.data = p.data - lr*grad\n",
        "                        p.grad.zero_()     #  zero out all gradients                   \n",
        "\n",
        "            loss_train += loss.item()\n",
        "\n",
        "        if epoch == 1 or epoch % 5 == 0:\n",
        "            print('{}  |  Epoch {}  |  Training loss {:.3f}'.format(\n",
        "                datetime.now().time(), epoch,\n",
        "                loss_train / len(train_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jiufDJzvWvS0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY6Z9-QeJh6J"
      },
      "source": [
        "5. Train 2 instances of MyMLP, one using train and the other using train manual update (use\n",
        "the same parameter values for both models). Compare their respective training losses. To get\n",
        "exactly the same results with both functions, see section 3.3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2A6L_QjYWqq4"
      },
      "outputs": [],
      "source": [
        "def compute_accuracy(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in loader:\n",
        "            imgs = imgs.to(device=device)\n",
        "            labels = labels.to(device=device)\n",
        "\n",
        "            outputs = model(imgs)\n",
        "            _, predicted = torch.max(outputs, dim=1)\n",
        "            total += labels.shape[0]\n",
        "            correct += int((predicted == labels).sum())\n",
        "\n",
        "    acc =  correct / total\n",
        "    print(\"Accuracy: {:.2f}\".format(acc))\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVE8CSxJJh6L",
        "outputId": "d449c96e-89ca-422f-bda8-3ceb6ea6f160"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of parameters:  1643234\n",
            "Number of parameter per layer:  [1572864, 512, 65536, 128, 4096, 32, 64, 2]\n",
            "Output: \n",
            " tensor([[-0.0430,  0.0806]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Now we can instantiate a model with the architecture defined in the cell above\n",
        "model = MyMLP()\n",
        "\n",
        "# Our model can be inspected exactly as we inspected our model in the previous tutorial (which was then defined using nn.Sequential) \n",
        "numel_list = [p.numel() for p in model.parameters()]\n",
        "print(\"Total number of parameters: \", sum(numel_list))\n",
        "print(\"Number of parameter per layer: \", numel_list)\n",
        "\n",
        "img, _ = cifar2_train[0]\n",
        "# Again we can feed a input and get the output exactly the same way as before\n",
        "output_tensor = model(img.unsqueeze(0))\n",
        "print(\"Output: \\n\", output_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaZKsX_rKer-",
        "outputId": "b8297437-4af8-40c8-c2a7-e5c2ecb9e0bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20:20:23.377197  |  Epoch 1  |  Training loss 0.681\n",
            "20:20:36.816051  |  Epoch 5  |  Training loss 0.553\n",
            "20:20:51.805633  |  Epoch 10  |  Training loss 0.472\n",
            "20:21:07.123797  |  Epoch 15  |  Training loss 0.425\n",
            "20:21:22.501640  |  Epoch 20  |  Training loss 0.388\n",
            "20:21:37.913883  |  Epoch 25  |  Training loss 0.355\n",
            "20:21:53.114117  |  Epoch 30  |  Training loss 0.326\n"
          ]
        }
      ],
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar2_train, batch_size=batch_size, shuffle=True)\n",
        "model1 = MyMLP().to(device=device) \n",
        "optimizer = optim.SGD(model1.parameters(), lr=1e-2)\n",
        "\n",
        "train(\n",
        "    n_epochs = n_epoch,\n",
        "    optimizer = optimizer,\n",
        "    model = model1,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0kd3laaJh6M",
        "outputId": "5e9ecb6d-18eb-4cb0-af04-289ca0851095"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20:21:56.343620  |  Epoch 1  |  Training loss 0.675\n",
            "20:22:09.038886  |  Epoch 5  |  Training loss 0.532\n",
            "20:22:24.727741  |  Epoch 10  |  Training loss 0.456\n",
            "20:22:40.234515  |  Epoch 15  |  Training loss 0.413\n",
            "20:22:55.796016  |  Epoch 20  |  Training loss 0.375\n",
            "20:23:11.315099  |  Epoch 25  |  Training loss 0.345\n",
            "20:23:26.881493  |  Epoch 30  |  Training loss 0.318\n"
          ]
        }
      ],
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar2_train, batch_size=batch_size, shuffle=True)\n",
        "model2 = MyMLP().to(device=device) \n",
        "lr = 1e-2\n",
        "\n",
        "train_manual(\n",
        "    n_epochs = n_epoch,\n",
        "    model = model2,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        "    lr = lr,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "N7zTTiruJh6O"
      },
      "outputs": [],
      "source": [
        "def train_manual(n_epochs, model, loss_fn, train_loader, lr=1e-2):\n",
        "    model.train()\n",
        "    \n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss_train = 0.0\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs = imgs.to(device=device) \n",
        "            labels = labels.to(device=device)\n",
        "\n",
        "            outputs = model(imgs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for i, p in enumerate(model.parameters()):\n",
        "                    if p.grad is not None:\n",
        "                        grad = p.grad.detach()\n",
        "                        p.data = p.data - lr*grad\n",
        "                        p.grad.zero_()     #  zero out all gradients                   \n",
        "\n",
        "            loss_train += loss.item()\n",
        "\n",
        "        if epoch == 1 or epoch % 5 == 0:\n",
        "            print('{}  |  Epoch {}  |  Training loss {:.3f}'.format(\n",
        "                datetime.now().time(), epoch,\n",
        "                loss_train / len(train_loader)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDGwgvfnJh6N"
      },
      "source": [
        "6. Modify train manual update by adding a L2 regularization term in your manual parameter\n",
        "update. Add an additional weight decay parameter to train manual update. Compare\n",
        "again train and train manual update results with 0 < weight decay < 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XkRcjx1CJh6P"
      },
      "outputs": [],
      "source": [
        "def train_manual_w_decay(n_epochs, model, loss_fn, train_loader, lr=1e-2, w_decay=0.):\n",
        "    model.train()\n",
        " \n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss_train = 0.0\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs = imgs.to(device=device) \n",
        "            labels = labels.to(device=device)\n",
        "\n",
        "            outputs = model(imgs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for i, p in enumerate(model.parameters()):\n",
        "                    if p.grad is not None:\n",
        "                        grad = p.grad.detach()\n",
        "                                            \n",
        "                        grad = grad + w_decay*p.data # L2 regularization term\n",
        "\n",
        "                        p.data = p.data - lr*grad\n",
        "                        p.grad.zero_()     #  zero out all gradients                   \n",
        "\n",
        "            loss_train += loss.item()\n",
        "\n",
        "        if epoch == 1 or epoch % 5 == 0:\n",
        "            print('{}  |  Epoch {}  |  Training loss {:.3f}'.format(\n",
        "                datetime.now().time(), epoch,\n",
        "                loss_train / len(train_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kk3rgd5uiSXp",
        "outputId": "fccdb877-a1f4-460a-b00f-0bb291709669"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20:23:30.126481  |  Epoch 1  |  Training loss 0.678\n",
            "20:23:43.132679  |  Epoch 5  |  Training loss 0.561\n",
            "20:23:59.186072  |  Epoch 10  |  Training loss 0.480\n",
            "20:24:15.139791  |  Epoch 15  |  Training loss 0.439\n",
            "20:24:31.131823  |  Epoch 20  |  Training loss 0.411\n",
            "20:24:47.200158  |  Epoch 25  |  Training loss 0.382\n",
            "20:25:03.849528  |  Epoch 30  |  Training loss 0.359\n"
          ]
        }
      ],
      "source": [
        "model3 = MyMLP().to(device=device) \n",
        "lr = 1e-2\n",
        "w_decay = 0.02\n",
        "\n",
        "train_manual_w_decay(\n",
        "    n_epochs = n_epoch,\n",
        "    model = model3,\n",
        "    loss_fn = loss_fn,\n",
        "    w_decay = w_decay,\n",
        "    train_loader = train_loader,\n",
        "    lr = lr,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzYcbTb0Jh6O"
      },
      "source": [
        "7. Modify train manual update by adding a momentum term in your parameter update. Add\n",
        "an additional momentum parameter to train manual update. Check again the correctness of\n",
        "the new update rule by comparing it to train function (with 0 < momentum < 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mxqGJwe6irNj"
      },
      "outputs": [],
      "source": [
        "def train_manual_w_decay_mom(n_epochs, model, loss_fn, train_loader, lr=1e-2, w_decay=0.,c_mom=0.):\n",
        "    model.train()\n",
        "    \n",
        "    mom_lst = []\n",
        "    curr_mom_lst = []\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss_train = 0.0\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs = imgs.to(device=device) \n",
        "            labels = labels.to(device=device)\n",
        "\n",
        "            outputs = model(imgs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for i, p in enumerate(model.parameters()):\n",
        "                    if p.grad is not None:\n",
        "                        grad = p.grad.detach()\n",
        "                                            \n",
        "                        grad = grad + w_decay*p.data # L2 regularization term\n",
        "\n",
        "                        if mom_lst == []:  # first time\n",
        "                          if curr_mom_lst == []:\n",
        "                            curr_mom_lst = [grad]\n",
        "                          else:\n",
        "                            curr_mom_lst.append(grad)\n",
        "                        else:\n",
        "                            grad = grad + mom_lst[i]*c_mom # build up some momentum!\n",
        "                            mom_lst[i] = grad\n",
        "\n",
        "                        p.data = p.data - lr*grad\n",
        "                        p.grad.zero_()     #  zero out all gradients\n",
        "                   \n",
        "            loss_train += loss.item()\n",
        "\n",
        "        if epoch == 1 or epoch % 5 == 0:\n",
        "            print('{}  |  Epoch {}  |  Training loss {:.3f}'.format(\n",
        "                datetime.now().time(), epoch,\n",
        "                loss_train / len(train_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkJGRYjbkx-C",
        "outputId": "94a058f3-1dc4-41f6-be2d-e6545592482f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20:25:07.382203  |  Epoch 1  |  Training loss 0.684\n",
            "20:25:20.413573  |  Epoch 5  |  Training loss 0.585\n",
            "20:25:36.474543  |  Epoch 10  |  Training loss 0.496\n",
            "20:25:52.659864  |  Epoch 15  |  Training loss 0.452\n",
            "20:26:10.106236  |  Epoch 20  |  Training loss 0.420\n",
            "20:26:31.571426  |  Epoch 25  |  Training loss 0.393\n",
            "20:26:48.716245  |  Epoch 30  |  Training loss 0.368\n"
          ]
        }
      ],
      "source": [
        "model4 = MyMLP().to(device=device) \n",
        "lr = 1e-2\n",
        "w_decay = 0.02\n",
        "c_mom = 0.75\n",
        "\n",
        "train_manual_w_decay_mom(\n",
        "    n_epochs = n_epoch,\n",
        "    model = model4,\n",
        "    loss_fn = loss_fn,\n",
        "    w_decay = w_decay,\n",
        "    c_mom = c_mom,\n",
        "    train_loader = train_loader,\n",
        "    lr = lr,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "D50gJE5KJh6S"
      },
      "outputs": [],
      "source": [
        "def model_val(model, train_loader, val_loader):\n",
        "    model.eval()\n",
        "    accs = {}\n",
        "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in loader:\n",
        "                imgs = imgs.to(device=device)\n",
        "                labels = labels.to(device=device)\n",
        "\n",
        "                outputs = model(imgs)\n",
        "                _, predicted = torch.max(outputs, dim=1)\n",
        "                total += labels.shape[0]\n",
        "                correct += int((predicted == labels).sum())\n",
        "\n",
        "        print(\"Acc {}: {:.3f}\".format(name , correct / total))\n",
        "        accs[name] = correct / total\n",
        "    return accs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyKeW9TrortX",
        "outputId": "a6a4ddf7-13a8-45f3-ebe0-298cd36d45f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Torch auto mode: model1\n",
            "Acc train: 0.880\n",
            "Acc val: 0.842\n",
            "\n",
            " Manual train: model2\n",
            "Acc train: 0.888\n",
            "Acc val: 0.845\n",
            "\n",
            " Manual train with weight decay: model3\n",
            "Acc train: 0.865\n",
            "Acc val: 0.829\n",
            "\n",
            " Manual train with weight decay + momentum: model4\n",
            "Acc train: 0.858\n",
            "Acc val: 0.813\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'train': 0.8576023067539092, 'val': 0.8128179043743642}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_load = torch.utils.data.DataLoader(cifar2_train, batch_size=batch_size, shuffle=False)\n",
        "val_load = torch.utils.data.DataLoader(cifar2_val, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(\"\\n Torch auto mode: model1\")\n",
        "model_val(model1, train_load, val_load)\n",
        "print(\"\\n Manual train: model2\")\n",
        "model_val(model2, train_load, val_load)\n",
        "print(\"\\n Manual train with weight decay: model3\")\n",
        "model_val(model3, train_load, val_load)\n",
        "print(\"\\n Manual train with weight decay + momentum: model4\")\n",
        "model_val(model4, train_load, val_load)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBZh5XCSJh6R"
      },
      "source": [
        "8. Train different instances (at least 4) of the MyMLP model with different learning rate, momentum\n",
        "and weight decay values . You can choose the same values as in the\n",
        "gradient descent output.txt file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7aPE1Qmw8Vw"
      },
      "source": [
        "I run out of memory with too many epochs in Google colab, 15 works.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhE01OPhqQlP",
        "outputId": "05281edd-e976-4897-e51e-d5c26397b05c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20:39:48.950942  |  Epoch 1  |  Training loss 0.670\n",
            "20:40:01.802657  |  Epoch 5  |  Training loss 0.564\n",
            "20:40:17.945280  |  Epoch 10  |  Training loss 0.494\n",
            "20:40:33.657585  |  Epoch 15  |  Training loss 0.446\n"
          ]
        }
      ],
      "source": [
        "model41 = MyMLP().to(device=device) \n",
        "lr = 0.01\n",
        "w_decay = 0\n",
        "c_mom = 0\n",
        "\n",
        "train_manual_w_decay_mom(\n",
        "    n_epochs = 15,\n",
        "    model = model41,\n",
        "    loss_fn = loss_fn,\n",
        "    w_decay = w_decay,\n",
        "    c_mom = c_mom,\n",
        "    train_loader = train_loader,\n",
        "    lr = lr,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGQTIFcsqdue",
        "outputId": "dfcd5577-2bd7-4a40-be50-0a42ea510272"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20:40:37.057725  |  Epoch 1  |  Training loss 0.673\n",
            "20:40:49.713985  |  Epoch 5  |  Training loss 0.555\n",
            "20:41:05.600759  |  Epoch 10  |  Training loss 0.481\n",
            "20:41:21.336818  |  Epoch 15  |  Training loss 0.440\n"
          ]
        }
      ],
      "source": [
        "model42 = MyMLP().to(device=device) \n",
        "lr = 0.01\n",
        "w_decay = 0.01\n",
        "c_mom = 0\n",
        "\n",
        "train_manual_w_decay_mom(\n",
        "    n_epochs = 15,\n",
        "    model = model42,\n",
        "    loss_fn = loss_fn,\n",
        "    w_decay = w_decay,\n",
        "    c_mom = c_mom,\n",
        "    train_loader = train_loader,\n",
        "    lr = lr,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPdP_RcIqe2X",
        "outputId": "00c6eedf-360d-404d-e159-cdc9e3d2801a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20:41:24.686031  |  Epoch 1  |  Training loss 0.686\n",
            "20:41:37.314978  |  Epoch 5  |  Training loss 0.606\n",
            "20:41:53.074532  |  Epoch 10  |  Training loss 0.504\n",
            "20:42:08.867373  |  Epoch 15  |  Training loss 0.454\n"
          ]
        }
      ],
      "source": [
        "model43 = MyMLP().to(device=device) \n",
        "lr = 0.01\n",
        "w_decay = 0.001\n",
        "c_mom = 0.9\n",
        "\n",
        "train_manual_w_decay_mom(\n",
        "    n_epochs = 15,\n",
        "    model = model43,\n",
        "    loss_fn = loss_fn,\n",
        "    w_decay = w_decay,\n",
        "    c_mom = c_mom,\n",
        "    train_loader = train_loader,\n",
        "    lr = lr,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5uIbuWdqhN0",
        "outputId": "3e89bc17-b132-4fb6-ba81-cb89b4ca1282"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20:42:12.225143  |  Epoch 1  |  Training loss 0.679\n",
            "20:42:24.915765  |  Epoch 5  |  Training loss 0.563\n",
            "20:42:40.854527  |  Epoch 10  |  Training loss 0.477\n",
            "20:42:56.945045  |  Epoch 15  |  Training loss 0.438\n"
          ]
        }
      ],
      "source": [
        "model44 = MyMLP().to(device=device) \n",
        "lr = 0.01\n",
        "w_decay = 0.01\n",
        "c_mom = 0.8\n",
        "\n",
        "train_manual_w_decay_mom(\n",
        "    n_epochs = 15,\n",
        "    model = model44,\n",
        "    loss_fn = loss_fn,\n",
        "    w_decay = w_decay,\n",
        "    c_mom = c_mom,\n",
        "    train_loader = train_loader,\n",
        "    lr = lr,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjK1SnJTugah",
        "outputId": "90cc0960-577f-4cec-e7d8-a818fe66d735"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Manual train with weight decay + momentum: model41\n",
            "Acc train: 0.819\n",
            "Acc val: 0.811\n",
            "\n",
            " Manual train with weight decay + momentum: model42\n",
            "Acc train: 0.818\n",
            "Acc val: 0.804\n",
            "\n",
            " Manual train with weight decay + momentum: model43\n",
            "Acc train: 0.811\n",
            "Acc val: 0.805\n",
            "\n",
            " Manual train with weight decay + momentum: model44\n",
            "Acc train: 0.819\n",
            "Acc val: 0.811\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'train': 0.8192303426860374, 'val': 0.8107833163784334}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"\\n Manual train with weight decay + momentum: model41\")\n",
        "model_val(model41, train_load, val_load)\n",
        "print(\"\\n Manual train with weight decay + momentum: model42\")\n",
        "model_val(model42, train_load, val_load)\n",
        "print(\"\\n Manual train with weight decay + momentum: model43\")\n",
        "model_val(model43, train_load, val_load)\n",
        "print(\"\\n Manual train with weight decay + momentum: model44\")\n",
        "model_val(model44, train_load, val_load)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMgVnFsIJh6T"
      },
      "source": [
        "• Wrap your computations inside a ”with torch.no grad():” context.\n",
        "• Remember that trainable parameters can be accessed using ”for p in model.parameters()”\n",
        "or ”for name, p in model.named parameters()”.\n",
        "• Remember that parameter values can then be accessed using ”p.data” and their gradients\n",
        "using ”p.grad”.\n",
        "• Gradient descent rules with L2-regularization and momentum can be found in the documentation of torch.optim.SGD.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "FGaGWIw77SHb"
      },
      "outputs": [],
      "source": [
        "def model_test(model, test_loader):\n",
        "    model.eval()\n",
        "    accs = {}\n",
        "    for name, loader in [(\"TEST\", test_loader)]:\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in loader:\n",
        "                imgs = imgs.to(device=device)\n",
        "                labels = labels.to(device=device)\n",
        "\n",
        "                outputs = model(imgs)\n",
        "                _, predicted = torch.max(outputs, dim=1)\n",
        "                total += labels.shape[0]\n",
        "                correct += int((predicted == labels).sum())\n",
        "\n",
        "        print(\"Test Acc {}: {:.3f}\".format(name , correct / total))\n",
        "        accs[name] = correct / total\n",
        "    return accs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Auftn0A960Zx",
        "outputId": "779dd887-505f-48a7-ed1c-a42b3de4f54e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Manual train with weight decay + momentum: model44\n",
            "Test Acc TEST: 0.822\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'TEST': 0.8215}"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Pick model44 as best\n",
        "'''\n",
        " Manual train with weight decay + momentum: model44\n",
        "Acc train: 0.819\n",
        "Acc val: 0.811\n",
        "{'train': 0.8192303426860374, 'val': 0.8107833163784334}'''\n",
        "\n",
        "test_load = torch.utils.data.DataLoader(cifar2_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "cifar2_test\n",
        "\n",
        "print(\"\\n Manual train with weight decay + momentum: model44\")\n",
        "model_test(model44, test_load)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "gradient_descent.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "c314c87e776466ef4f077dc585c9dc3eaee46cc84096b0d5b81aa8c7c1fafbb2"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('ai')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

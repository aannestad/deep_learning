{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0b4cdbc7c3fa968f8f57a8da50a47c3ee9b8594a4de967130c021b1087ec3a6ed",
   "display_name": "Python 3.8.5 64-bit ('ml': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### Implement softmax and compare it to torch.softmax for torch.nn\n",
    "\n",
    "**Softmax** is S(y_i) = exp(y_i) / sum(exp(y_j))"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmaks(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "softmaks: [0.87942377 0.0721875  0.04838873]\n"
     ]
    }
   ],
   "source": [
    "test = np.array([3.0, 0.5, 0.1])\n",
    "print(\"softmaks:\", softmaks(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.softmax:  tensor([0.8794, 0.0722, 0.0484], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "test_tensor = torch.tensor(test)\n",
    "print(\"torch.softmax: \", torch.softmax(test_tensor,dim=0))"
   ]
  },
  {
   "source": [
    "Impement ** Cross-entropy ** and compare it to torch.Cross_entropy\n",
    "\n",
    "D(Y_hat, Y) = -1/N ** sum(Y_j * log(Y_hat_j))\n",
    "\n",
    "Loss increases if predicted probability diverges from actual label\n",
    "Better prediction (less divergence) -> lower loss\n",
    "\n",
    "One-hot encoded label, e.g. Y=[[1,0,0]], Y_hat=[[0.7,0.2,0.1]]'"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kryss_entropi(y, y_hat):\n",
    "    tap = -np.sum(y * np.log(y_hat))\n",
    "    return tap # /y_hat.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Good prediction: 0.36\nBad prediction: 2.30\n"
     ]
    }
   ],
   "source": [
    "Y = np.array([1,0,0])\n",
    "Y_pred1 = np.array([0.7,0.2,0.1])\n",
    "Y_pred2 = np.array([0.1,0.3,0.6])\n",
    "\n",
    "# First prediction should have a low loss (cross entropy), second should have a high loss\n",
    "\n",
    "print(f'Good prediction: {kryss_entropi(Y,Y_pred1):.2f}')\n",
    "print(f'Bad prediction: {kryss_entropi(Y,Y_pred2):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Correct class labels: tensor([2, 0])\nPredicted class (good pred): tensor([2, 0])\nPredicted class (bad pred): tensor([1, 2])\nGood prediction: 0.17\nBad prediction: 2.48\n"
     ]
    }
   ],
   "source": [
    "# Cross-entropy with pytorch\n",
    "import torch.nn as nn\n",
    "\n",
    "# convert arrays to tensors\n",
    "# Let Y have labes (classes): 0, 1, 2 (e.g Red, Green, Blue)\n",
    "\n",
    "Y = torch.tensor([2,0]) # 2 examples (with correct class labels (not one-hot as above))\n",
    "\n",
    "# n_samples x m_classes = 2x3\n",
    "# (use raw values (softmax is included in pytorch Loss function))\n",
    "Y_pred1 = torch.tensor([[0.1, 0.2, 2.1], [3.1, 0.08, 0.2]])    # good prediction\n",
    "Y_pred2 = torch.tensor([[0.5, 3.1, 0.3], [0.1, 0.2, 1.8]])     # bad prediction\n",
    "\n",
    "loss = nn.CrossEntropyLoss()   # select loss function\n",
    "\n",
    "loss1 = loss(Y_pred1, Y)\n",
    "loss2 = loss(Y_pred2, Y)\n",
    "\n",
    "print(f'Correct class labels: {Y}')\n",
    "\n",
    "print(f'Predicted class (good pred): {torch.max(Y_pred1, 1)[1]}')\n",
    "print(f'Predicted class (bad pred): {torch.max(Y_pred2, 1)[1]}')\n",
    "\n",
    "print(f'Good prediction: {loss1:.2f}')\n",
    "print(f'Bad prediction: {loss2:.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example (multiclass) network\n",
    "\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_classes):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.lin1 = nn.Linear(input_size, hidden_size)\n",
    "        self.lin2 = nn.Linear(hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.lin1(x))     # Activation after layer 1\n",
    "        out = self.lin2(out)\n",
    "        return out\n",
    "\n",
    "# instantiate network\n",
    "model = MyNet(input_size=28*28, hidden_size=5, n_classes=3)\n",
    "crit = nn.CrossEntropyLoss()  # has softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example (binary classification) network\n",
    "\n",
    "class BinNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):       # no n_classes\n",
    "        super(BinNet, self).__init__()\n",
    "        self.lin1 = nn.Linear(input_size, hidden_size) # Layer 1 (in_features, out_features)\n",
    "        self.relu = nn.ReLU()                          # Activation after layer 1\n",
    "        self.lin2 = nn.Linear(hidden_size, 1)          # Layer 2 (only 1 class out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.lin1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.lin2(out)\n",
    "        y_pred = torch.sigmoid(out)\n",
    "        return y_pred\n",
    "\n",
    "#instantiate network\n",
    "bin_Model = BinNet(input_size=28*28, hidden_size=5) # no n_class inputs\n",
    "crit = nn.BCELoss()   # binary cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
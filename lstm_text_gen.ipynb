{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0b4cdbc7c3fa968f8f57a8da50a47c3ee9b8594a4de967130c021b1087ec3a6ed",
   "display_name": "Python 3.8.5 64-bit ('ml': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## INF265 - Weeks 14-15: LSTM-based character level language models\n",
    "## By Hans Martin Aannestad\n",
    "\n",
    "*1. Download a text document that you like. The project Gutenberg is a\n",
    "good place to obtain legally interesting books !\n",
    "\n",
    "COMMENT: To see if the model can generate a specific writing style, I follow the tradition and use the complete works of Shakespeare, downloaded from Gutenberg.org"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*2. Load and clean up the text. You will remove all accents, force all letters\n",
    "to be lower case, remove all occurrences of ”\\n”, ”\\t” and ”\\r” and make\n",
    "sure every two consecutive words in the text are separated by a single\n",
    "space ” ”. You can also remove part (or all) of the punctuation.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as t\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import unicodedata\n",
    "import string\n",
    "import os\n",
    "\n",
    "text=open(\"shakespeare.txt\",encoding=\"utf8\").read().strip().lower().split()\n",
    "words=[]\n",
    "\n",
    "#vocab = string.ascii_letters + \" .,;'\"\n",
    "#n = len(vocab)\n",
    "\n",
    "# normalize text\n",
    "for word in text[:100000]:\n",
    "     words.append(''.join(char for char in unicodedata.normalize('NFD', word) if   unicodedata.category(char) != 'Mn' and char in string.ascii_letters + \" .,;'\"))\n",
    "\n",
    "text = \" \".join(words)\n",
    "text = text.replace(';','').replace(':','').replace(',','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'ll pointing to each his thunder rain and wind or say with princes if it shall go well by oft predict that i in heaven find. but from thine eyes my knowledge i derive and constant stars in them i read such art as truth and beauty shall together thrive if from thy self to store thou wouldst convert or else of thee this i prognosticate thy end is truths and beautys doom and date.  when i consider every thing that grows holds in perfection but a little moment. that this huge stage presenteth nought '"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# Inspect some Shakespeare\n",
    "text[10000:10500]"
   ]
  },
  {
   "source": [
    "*3. Create a character-based vocabulary from the preprocessed text: every appearing character will have a unique integer ID. You can use a dictionary structure to store the vocabulary."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "v = list(''.join(set(text)))\n",
    "v = sorted(v)\n",
    "n_chars = len(v)\n",
    "d = {}\n",
    "for i in range(0,len(v)):\n",
    "    d[i] = v[i]\n",
    "\n",
    "chars = tuple(sorted(set(text)))\n",
    "char_int = dict([(c,i) for i,c in enumerate(chars)])\n",
    "int_char = dict(enumerate(chars)) # (Invert) model ints back to characters\n",
    "int_text = [char_int[char] for char in text] # Convert text to int ID\n",
    "\n",
    "# To demonstrate strategy: encode a single one-hot converted text\n",
    "L = 25\n",
    "Y = t.zeros(1)\n",
    "i = 0\n",
    "\n",
    "label = np.array([[c] for c in int_text[i:i+L]])\n",
    "label_t = t.LongTensor(label)\n",
    "x_one_hot = t.zeros(L, n_chars).scatter_(1,label_t,1)\n",
    "Y[0] = int_text[i+L]"
   ]
  },
  {
   "source": [
    "*4. Generate the data (X, y) for a character level language model:\n",
    "(a) Decide of a sequence length L."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(text)\n",
    "\n",
    "X = t.zeros(N,L,n_chars)\n",
    "Y = t.zeros(N,dtype=t.int64)\n",
    "\n",
    "for i in range(N-L):\n",
    "    label = np.array([[c] for c in int_text[i:i+L]])\n",
    "    label_t = t.LongTensor(label)\n",
    "    X[i] = t.zeros(L, n_chars).scatter_(1,label_t,1)\n",
    "    Y[i] = int_text[i+L]"
   ]
  },
  {
   "source": [
    "*5. Shuffle then split the data between train and validation sets. You probably\n",
    "want to keep most of the data available for training."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "ds = TensorDataset(X, Y)\n",
    "\n",
    "train_len = int(len(ds)*0.9)\n",
    "ds_train, ds_test = random_split(ds, [train_len, len(ds)-train_len])\n",
    "\n",
    "batch_size = 2048\n",
    "\n",
    "train_loader = DataLoader(ds_train, batch_size=batch_size)\n",
    "#test_loader = DataLoader(ds_test, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "input_size = n_chars\n",
    "seq_length = 25\n",
    "num_layers = 1\n",
    "\n",
    "hidden_size = 256 # via rute of thumb << N/(10*(inputs+outputs))\n",
    "num_classes = n_chars\n",
    "num_epochs = 10 # Due to time constraints\n",
    "learning_rate = 0.001 # By industry conventions"
   ]
  },
  {
   "source": [
    "*6. Implement a LSTM classifier designed to predict the next character from\n",
    "a sequence of L consecutive characters. A good starting point would be to\n",
    "stack a ’torch.nn.LSTM()’ module followed by a ’torch.nn.Linear()’ layer.\n",
    "Hint: For this task, we need a ”many-to-one” type of LSTM."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.Net(input_size, hidden_size, num_layers, batch_first = True)\n",
    "\n",
    "        # x -> batch_size, seq, input_size\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, num_classes) # fully connected\n",
    "\n",
    "    def forward(self, x):\n",
    "        # initial hidden state (not used)\n",
    "        #h0 = t.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "\n",
    "        # initial cell state (not used)\n",
    "        #c0 = t.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "\n",
    "        out, (h,c) = self.lstm(x) #, (h0, c0))\n",
    "\n",
    "        # out shape: batch_size, seq_length, hidden_size\n",
    "    \n",
    "        out = self.fc(h) # Since n_layers = 1 we can use hidden state as input in linear layer\n",
    "        return out.squeeze()"
   ]
  },
  {
   "source": [
    "*7. Implement the training loop. You will choose carefully the loss function\n",
    "and a metric that are suitable for the task. Your code will print the loss\n",
    "and the chosen metric at the end of every epoch, both for the train and\n",
    "the validation sets."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "num steps: 232\n",
      "C:\\Users\\hmaan\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\autograd\\__init__.py:130: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  Variable._execution_engine.run_backward(\n",
      "Epoch [1/10], Step [232/232], Loss: 2.6230\n",
      "Epoch [2/10], Step [232/232], Loss: 2.1562\n",
      "Epoch [3/10], Step [232/232], Loss: 2.0116\n",
      "Epoch [4/10], Step [232/232], Loss: 1.9034\n",
      "Epoch [5/10], Step [232/232], Loss: 1.8123\n",
      "Epoch [6/10], Step [232/232], Loss: 1.7360\n",
      "Epoch [7/10], Step [232/232], Loss: 1.6727\n",
      "Epoch [8/10], Step [232/232], Loss: 1.6207\n",
      "Epoch [9/10], Step [232/232], Loss: 1.5772\n",
      "Epoch [10/10], Step [232/232], Loss: 1.5402\n"
     ]
    }
   ],
   "source": [
    "# instantiate model\n",
    "model = Net(input_size, hidden_size, num_layers, num_classes)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = t.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "\n",
    "n_steps = len(train_loader)            # num examples\n",
    "print(\"num steps:\",n_steps)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for i, (samples, labels) in enumerate(train_loader):\n",
    "        #print(sample[0])\n",
    "\n",
    "        # forward\n",
    "        outputs = model(samples)           # 1: predict\n",
    "        loss = criterion(outputs, labels)  # 2: calculate loss\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()   # 1: empty vals in gradient\n",
    "        loss.backward()         # 2: backpropagation\n",
    "        optimizer.step()        # 3: update parameters\n",
    "\n",
    "        # print progress info\n",
    "        #if (i+1) % 100 == 0:\n",
    "    print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_steps}], Loss: {epoch_loss/n_steps:.4f}')\n",
    "\n",
    "# Save, load, run\n",
    "#t.save(model,\"trained\")\n",
    "#m1=t.load(\"trained\")\n",
    "#examples = iter(train_loader)\n",
    "#samples, lables = examples.next()\n",
    "#print(m1(samples))"
   ]
  },
  {
   "source": [
    "*8. Finetune your model (for simplicity, no model selection pipeline required\n",
    "in this exercise): you will play with the hyperparameters of your model,\n",
    "including (non-exclusively) the sequence length, the hidden size of the\n",
    "LSTM module, the number of layers in the LSTM module, the batch size\n",
    "and the weight decay.\n",
    "\n",
    "ANSWER: Batch size and sequence sequence length and number of epochs as stated above in final version"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*9. Train your model and analyze its performance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Correct predicted / total = 27839 / 52671\nPrediction accuracy = 0.5285\n"
     ]
    }
   ],
   "source": [
    "# Validate\n",
    "\n",
    "test_loader = DataLoader(ds_test, batch_size=1) # no batching\n",
    "n_samples = len(test_loader)\n",
    "\n",
    "n_correct = 0\n",
    "\n",
    "with t.no_grad():\n",
    "    for sample, label in test_loader:\n",
    "        #print(sample)\n",
    "        #print(sample.shape)\n",
    "  \n",
    "        output = model(sample)\n",
    "        _, pred = t.max(output, dim = 0)\n",
    "        n_correct += int(pred == label)\n",
    "\n",
    "    acc = n_correct / n_samples\n",
    "    print(f'Correct predicted / total = {n_correct} / {n_samples}')\n",
    "    print(f'Prediction accuracy = {acc:.4f}')"
   ]
  },
  {
   "source": [
    "*10. Use your trained model to generate new text:\n",
    "(a) Choose a ”seed sequence” consisting of L characters of your choice\n",
    "(among the characters in the vocabulary)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Seed sequence: his thunder rain and wind\n",
      "his thunder rain and wind that the sent the beart the world and the true that i have so the part of the world and the true that i have so the part of the world and the true that i have so the part of the world and the true th\n"
     ]
    }
   ],
   "source": [
    "seed_seq = \"his thunder rain and wind\"  # L=25\n",
    "print(\"Seed sequence: \" + \"his thunder rain and wind\")\n",
    "\n",
    "Y = t.zeros(1)\n",
    "int_seed_seq = [char_int[char] for char in seed_seq] # Convert text to int ID\n",
    "label = np.array([[c] for c in int_seed_seq ])\n",
    "label_t = t.LongTensor(label)\n",
    "x_one_hot = t.zeros(L, n_chars).scatter_(1,label_t,1).unsqueeze(dim=0)\n",
    "Y[0] = char_int[' ']  # next character intentionally left blank\n",
    "\n",
    "gen_text = seed_seq\n",
    "\n",
    "for i in range(200):\n",
    "    \n",
    "    #(c) Pass your encoded seed sequence through your trained model.\n",
    "    y_out = model(x_one_hot) #  (DataLoader(TensorDataset(X, Y), batch_size=1))\n",
    "\n",
    "    #(d) Predict the next character as the argmax of the softmax activation on the output.\n",
    "    pred_int = int(t.argmax(t.softmax(y_out,dim=0)).detach())\n",
    "    gen_text += int_char[pred_int]\n",
    "\n",
    "    #(e) One-hot encode the predicted character.\n",
    "    pred_hot = t.zeros(1,n_chars)\n",
    "    pred_hot[0][pred_int] = 1\n",
    "\n",
    "    #(e-f) Update the seed sequence: remove the encoded character in first position of the encoded seed sequence, then      add the encoded predicted character in last position of the encoded seed sequence.\n",
    "    x_one_hot=t.cat([x_one_hot[0][1:],pred_hot]).unsqueeze(dim=0)\n",
    "\n",
    "print(gen_text)\n"
   ]
  },
  {
   "source": [
    "*11. What problem seems to occur with the previous procedure ?\n",
    "\n",
    "ANSWER: The prediction started ok, however no variation will happen in the generated text (only repeating the (best) prediction)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*12. Modify the procedure described in Question 10: rather than predicting\n",
    "the next character as the argmax of the softmax activation on the output,\n",
    "you will instead sample it from the probability distribution given by the\n",
    "softmax activation on the output."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Seed sequence: his thunder rain and wind\n",
      "\n",
      "his thunder rain and wind tell it no homoured caesar shapparcowen dauch as their godaty see all or coed know my count dornof o to your wasters tadee. i sheesbry death till what shall noternessold gonce ill bains men lide it. wist hath benied you and held at dead and heart spike the combong non that i lutter i know my playe here to the bear i will i besere but dack and vessell and at add upenst foor dingratem delive and that and chayse at mine that kiend lay whure the brotwied me. thenefilewacly morrilled his diade and weld he than celfarity. wistrat mine early you shall be natles beine your bugwara well thou arstone with way nater have the stain and make hores grawer that i heagh and the camp. anowan oft thet himpekend and love i hate your sirve and repaie the gropid on the fachilar's happinast. his will truth hore to that thou beace yount. jurge a read me day nought make may creak layte in the shound antony. tongued upon the known ows ring cown ofth im sin. ofe this heart no camoon bected offleving. exeung ca\n"
     ]
    }
   ],
   "source": [
    "seed_seq = \"his thunder rain and wind\"  # L=25, \"hippolyta\" the unique word to be completed by prediction\n",
    "print(\"Seed sequence: \" + \"his thunder rain and wind\\n\")\n",
    "\n",
    "Y = t.zeros(1)\n",
    "int_seed_seq = [char_int[char] for char in seed_seq] # Convert text to int ID\n",
    "label = np.array([[c] for c in int_seed_seq ])\n",
    "label_t = t.LongTensor(label)\n",
    "x_one_hot = t.zeros(L, n_chars).scatter_(1,label_t,1).unsqueeze(dim=0)\n",
    "Y[0] = char_int[' ']  # next character intentionally left blank\n",
    "\n",
    "gen_text = seed_seq\n",
    "\n",
    "for i in range(1000):\n",
    "    \n",
    "    #(c) Pass your encoded seed sequence through your trained model.\n",
    "    y_out = model(x_one_hot) #  (DataLoader(TensorDataset(X, Y), batch_size=1))\n",
    "\n",
    "    #(d)* By random sampling, predict the next character as the argmax of the softmax activation on the output.\n",
    "    s_max = np.array(t.softmax(y_out,dim=0).detach().numpy()).astype('float64')\n",
    "    pred_int = np.argmax(np.random.multinomial(1,s_max/sum(s_max),1))\n",
    "\n",
    "    gen_text += int_char[pred_int]\n",
    "    \n",
    "    #(e) One-hot encode the predicted character.\n",
    "    pred_hot = t.zeros(1,n_chars)\n",
    "    pred_hot[0][pred_int] = 1\n",
    "\n",
    "    #(e-f) Update the seed sequence: remove the encoded character in first position of the encoded seed sequence, then      add the encoded predicted character in last position of the encoded seed sequence.\n",
    "    x_one_hot=t.cat([x_one_hot[0][1:],pred_hot]).unsqueeze(dim=0)\n",
    "    \n",
    "print(gen_text)"
   ]
  },
  {
   "source": [
    "*13. Entertain us by generating amusing text !"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}